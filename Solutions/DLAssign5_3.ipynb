{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "vVtgeae3h6K0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RujbUeWWxkFY"
      },
      "outputs": [],
      "source": [
        "# Define the self-attention layer\n",
        "class SelfAttention(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.gamma = tf.Variable(initial_value=tf.zeros((1,), dtype='float32'), trainable=True)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, height, width, channels = input_shape\n",
        "        self.query_conv = layers.Conv2D(channels // 8, kernel_size=1, use_bias=False)\n",
        "        self.key_conv = layers.Conv2D(channels // 8, kernel_size=1, use_bias=False)\n",
        "        self.value_conv = layers.Conv2D(channels, kernel_size=1, use_bias=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute the query, key, and value matrices\n",
        "        query = self.query_conv(inputs)\n",
        "        key = self.key_conv(inputs)\n",
        "        value = self.value_conv(inputs)\n",
        "\n",
        "        # Compute the dot-product attention\n",
        "        attention_logits = tf.matmul(query, key, transpose_b=True)\n",
        "        attention_weights = tf.nn.softmax(attention_logits)\n",
        "        attention_output = tf.matmul(attention_weights, value)\n",
        "\n",
        "        # Apply the gamma scaling factor\n",
        "        attention_output = self.gamma * attention_output\n",
        "\n",
        "        # Add the attention output to the original input\n",
        "        output = inputs + attention_output\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN with one self-attention layer\n",
        "def cnn_sa():\n",
        "    input_shape = (32, 32, 3)\n",
        "    num_classes = 10\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        SelfAttention(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        SelfAttention(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uQ8q0ji6h_F6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxEEfNI5yPrL",
        "outputId": "dcf2ff0a-b698-4348-a5a5-4b216f6ebc21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 417s 529ms/step - loss: 1.4350 - accuracy: 0.5115 - val_loss: 0.9623 - val_accuracy: 0.6647\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 407s 521ms/step - loss: 0.9281 - accuracy: 0.6775 - val_loss: 0.8805 - val_accuracy: 0.6942\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 405s 518ms/step - loss: 0.7825 - accuracy: 0.7299 - val_loss: 0.8013 - val_accuracy: 0.7204\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 400s 512ms/step - loss: 0.6898 - accuracy: 0.7621 - val_loss: 0.7198 - val_accuracy: 0.7504\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 403s 515ms/step - loss: 0.6172 - accuracy: 0.7879 - val_loss: 0.6879 - val_accuracy: 0.7597\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 408s 522ms/step - loss: 0.5526 - accuracy: 0.8096 - val_loss: 0.6456 - val_accuracy: 0.7778\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 402s 514ms/step - loss: 0.4996 - accuracy: 0.8268 - val_loss: 0.6407 - val_accuracy: 0.7789\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 403s 516ms/step - loss: 0.4491 - accuracy: 0.8456 - val_loss: 0.6695 - val_accuracy: 0.7758\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 401s 512ms/step - loss: 0.4082 - accuracy: 0.8557 - val_loss: 0.7712 - val_accuracy: 0.7422\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 406s 519ms/step - loss: 0.3747 - accuracy: 0.8689 - val_loss: 0.6655 - val_accuracy: 0.7888\n",
            "313/313 [==============================] - 20s 64ms/step - loss: 0.6655 - accuracy: 0.7888\n",
            "Test loss: 0.6655, Test accuracy: 0.7888\n"
          ]
        }
      ],
      "source": [
        "# loading cifar 10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = cnn_sa()\n",
        "\n",
        "# Compile and training the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}